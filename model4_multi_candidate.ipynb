{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import scipy.sparse as spr\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "\n",
    "pd.set_option( 'display.max.row' , 200 )\n",
    "\n",
    "\n",
    "genre = pd.read_json('data/genre_gn_all.json',typ = 'series' )\n",
    "train = pd.read_json( 'data/train.json'  , encoding = 'utf-8' )\n",
    "val = pd.read_json( 'data/val.json'  , encoding = 'utf-8' )\n",
    "song_meta = pd.read_json( 'data/song_meta.json')\n",
    "\n",
    "\n",
    "# 최종\n",
    "test = pd.read_json('data/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**분야별 집계함수 만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_matrix( df  , cls_data , by = 'song' ):\n",
    "    \"\"\"\n",
    "    # df = dataframe\n",
    "            play list \n",
    "    # cls_dat = dataframe\n",
    "            clustered data\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df\n",
    "    cls_data = cls_data\n",
    "\n",
    "    n_cluster = len(cls_data.label.unique()) \n",
    "    n_plylst = len( df )\n",
    "    cls_data = cls_data[ cls_data.columns[:2] ]\n",
    "\n",
    "    cl_val = dict( zip( cls_data.iloc[:,0] , cls_data.iloc[:,1]))\n",
    "        \n",
    "    if by == 'song':\n",
    "        df = df[['id' , 'songs']]\n",
    "        cl_ply_values = [ dict(Counter([ cl_val.get(str(v)) for v in vs])) for vs in df.songs ]\n",
    "        \n",
    "    elif by == 'word':\n",
    "        df = df[['id' , 'complex_col']]\n",
    "        cl_ply_values = [ dict(Counter([ cl_val.get(str(v)) for v in vs if cl_val.get(str(v)) != None])) for vs in df.complex_col ]\n",
    "    \n",
    "    elif by == 'singer':\n",
    "        df = df[['id' , 'complex_col']]\n",
    "        cl_ply_values = [ dict(Counter([ cl_val.get(str(v)) for v in vs if cl_val.get(str(v)) != None])) for vs in df.complex_col ]\n",
    "        \n",
    "    elif by == 'tag':\n",
    "        df = df[['id' , 'tags']].reset_index( drop = True )\n",
    "        cl_ply_values = [ dict(Counter([ cl_val.get(str(v)) for v in vs if cl_val.get(str(v)) != None])) for vs in df.tags ]\n",
    "\n",
    "    elif by == 'genre':\n",
    "        df = df[['id' , 'gnr']]\n",
    "        cl_ply_values = [ dict(Counter([ cl_val.get(str(v)) for v in vs if cl_val.get(str(v)) != None])) for vs in df.gnr ]\n",
    "\n",
    "    elif by == 'album':## add\n",
    "        df = df[['id' , 'albums']]\n",
    "        cl_ply_values = [ dict(Counter([ cl_val.get(str(v)) for v in vs if cl_val.get(str(v)) != None])) for vs in df.albums ]\n",
    "\n",
    "        \n",
    "    col_dot = [list(d.keys()) for d in cl_ply_values] \n",
    "    row = np.repeat( range( n_plylst )  , [ len( c ) for c in col_dot ] ) \n",
    "    col = [ c for cols in col_dot for c in cols ]\n",
    "    data = [ d for k in cl_ply_values for d in list( k.values() ) ]\n",
    "    matrix_cluster = spr.csr_matrix( ( data , ( row , col )) , shape = ( n_plylst , n_cluster ) )\n",
    "    \n",
    "    return pd.concat( [ df.id ,pd.DataFrame(matrix_cluster.toarray() , columns = [f'{by}_cl{i}' for i in range( n_cluster ) ] )]  ,axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3></h3>\n",
    "<h3></h3>\n",
    "\n",
    "### 후보군 선별하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사전에 날짜 거르기** \n",
    "\n",
    "후보군을 뽑을때, 플레이 리스트보다 날짜가 나중이라면, 그 노래를 제외한다.\n",
    "\n",
    "- 노래와 날짜가 매핑된 변수를 생성하고 ( get_song_date_dict ) -->  song_date!! 변수\n",
    "- 후보군을 뽑는과정 ( get_candidate ) 에서 걸러준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_date_dict( song_meta ): # song_meta를 받아서 날짜와 매핑한다.\n",
    "    # 간혹 20050200 같은 놈들은 20050101\n",
    "    # 0 인놈들은 1990 으로 변경\n",
    "    a = []\n",
    "    for t in song_meta.issue_date:\n",
    "        try:\n",
    "            a.append(datetime.datetime.strptime( str(t) , '%Y%m%d' ).date())\n",
    "        except:\n",
    "            try:\n",
    "                a.append(datetime.datetime.strptime( str(t)[:4] , '%Y' ).date())\n",
    "            except:\n",
    "                a.append(datetime.datetime.strptime( '1990' , '%Y' ).date())\n",
    "    return dict(zip(song_meta.id , a))\n",
    "\n",
    "song_date = get_song_date_dict(song_meta) ### \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3></h3>\n",
    "<h3></h3>\n",
    "\n",
    "**유사한 플레이리스트를 찾는 함수와, 플레이리스트가 담고 있는 노래, 태그들을 모아 주는 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_ply(val_ply , train_ply , top_ply = 30): # 유사도 높은 playlist뽑기\n",
    "    \"\"\"\n",
    "    val_ply : df\n",
    "        예측할 playlist ( 노래기반 val_song , word 기반 val_word , 둘다 val_ply )\n",
    "    train_ply : df\n",
    "        비교대상 playlist ( 노래기반 train_song , word 기반 train_word , 둘다 train_ply )\n",
    "    top_ply : int\n",
    "        상위 노래 갯수\n",
    "    \"\"\"\n",
    "    \n",
    "    n_train = len(train_ply)\n",
    "    n_val = len( val_ply )\n",
    "    train_key = dict(zip( range(n_train) , train_ply.id  ))\n",
    "    val_key = dict(zip( range(n_val) , val_ply.id  ))\n",
    "\n",
    "    sim_matrix = cosine_similarity(val_ply.set_index('id') , train_ply.set_index('id'))\n",
    "\n",
    "    sim_ply_dict = {}\n",
    "    for i , val_ in tqdm(enumerate(sim_matrix)):\n",
    "        sim_ply_dict[val_key[i]] = [train_key[p] for p in np.argsort(-val_)[:top_ply]]\n",
    "\n",
    "    return sim_ply_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3></h3>\n",
    "<h3></h3>\n",
    "\n",
    "**나름의 분산처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_get_sim_ply( start,inter , val_ply  , train_ply   , tn = 30 ):\n",
    "    res = {}\n",
    "    max_n = len(val_ply)\n",
    "    for i in range( start , max_n ,  inter ):\n",
    "        try:\n",
    "            tmp = get_sim_ply( val_ply = val_ply.iloc[i : i + inter] ,train_ply = train_ply , top_ply = tn )\n",
    "            res.update( tmp )\n",
    "            print(f'{i+inter} 만큼 했다')\n",
    "        except:\n",
    "            print( f'ERROR { i } 에서 Save')\n",
    "            return res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유틸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_pickle( ob , f_path ):\n",
    "    with open( f_path , 'wb') as f:\n",
    "        pickle.dump( ob , f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3></h3>\n",
    "<h3></h3>\n",
    "\n",
    "**song matrix만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_vec = pd.read_pickle( 'data/song_clustering/clust_song200.pickle') # train + val + test 기반 임베딩\n",
    "song_vec = song_vec[['song_id' , 'label'] + list(song_vec.columns[3:12])]\n",
    "\n",
    "train_song = get_cluster_matrix( train , song_vec , by ='song') # song ( train )\n",
    "val_song = get_cluster_matrix( val , song_vec , by ='song') # song ( val )\n",
    "\n",
    "# 최종!\n",
    "# test_song = get_cluster_matrix( test , song_vec  , by = 'song' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ply = dist_get_sim_ply( 0 ,  3000 , val_ply = val_song , train_ply = train_song , tn = 7 )\n",
    "save_to_pickle( sim_ply , 'candi/res_ver34_song.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3></h3>\n",
    "<h3></h3>\n",
    "\n",
    "**word matrix만들기**\n",
    "- tag + title + genre !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어들로 이루어진 ply df ( train + val )\n",
    "ply_words = pd.read_pickle( 'data/transfromation/tag_gnr_title_df.pickle')\n",
    "\n",
    "#임베딩된 개별 단어 df\n",
    "word_vec = pd.read_pickle( 'data/word_clustering/clu_tag_gnr_title_emb_100' )\n",
    "\n",
    "\n",
    "total_word = get_cluster_matrix( ply_words , word_vec , by = 'word')\n",
    "\n",
    "val_word = total_word.loc[ total_word.id.isin( list(val.id.values) ) ] # word ( val )\n",
    "train_word = total_word.loc[ total_word.id.isin( list(train.id.values ) ) ] # word ( \n",
    "\n",
    "# 최종!\n",
    "# test_word = total_word.loc[ total_word.id.isin( list(test.id.values ) ) ]\n",
    "# del total_word #( 메모리 최적화를 위해 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ply = dist_get_sim_ply( 0 ,  3000 , val_ply = val_word , train_ply = train_word , tn = 7 )\n",
    "save_to_pickle( sim_ply , 'candi/res_ver34_word.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3></h3>\n",
    "<h3></h3>\n",
    "\n",
    "**singer matrix만들기**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가수와 제목으로 이루어진 ply df ( train + val + test )\n",
    "ply_singer = pd.read_pickle( 'data/transfromation/title_singer_df.pickle')\n",
    "\n",
    "#임베딩된 개별 단어,가수\n",
    "singer_vec = pd.read_pickle( 'data/word_clustering/clu_singer_emb_100')\n",
    "\n",
    "\n",
    "total_singer = get_cluster_matrix( ply_singer , singer_vec , by = 'singer')\n",
    "\n",
    "val_singer = total_singer.loc[ total_singer.id.isin( list(val.id.values) ) ] # singer \n",
    "train_singer = total_singer.loc[ total_singer.id.isin( list(train.id.values) ) ] # sin\n",
    "\n",
    "#최종!\n",
    "# test_singer = total_singer.loc[ total_singer.id.isin( list(test.id.values) ) ] \n",
    "# del total_word # 메모리 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ply = dist_get_sim_ply( 0 ,  3000 , val_ply = val_singer , train_ply = train_singer , tn = 7 )\n",
    "save_to_pickle( sim_ply , 'candi/res_ver34_singer.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3></h3>\n",
    "<h3></h3>\n",
    "\n",
    "**tag matrix만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테그로만 이루어진 ply df\n",
    "tag_df = pd.read_pickle('data/transfromation/tag_df.pickle') # tag_df 에 train , val , test 모두 합친거 넣기\n",
    "\n",
    "# 임베딩된 태그들\n",
    "tag_vec = pd.read_pickle('data/word_clustering/clustering_tag_emb_30.pickle') # train , val , test 모두 합쳐서 embeding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tag = get_cluster_matrix( tag_df , tag_vec , by = 'tag')\n",
    "\n",
    "val_tag = total_tag.loc[ total_tag.id.isin( list(val.id.values) )]\n",
    "train_tag = total_tag.loc[ total_tag.id.isin( list(train.id.values) )]\n",
    "\n",
    "#최종\n",
    "# test_tag = total_tag.loc[ total_tag.id.isin( list(test.id.values) )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ply = dist_get_sim_ply( 0 ,  3000 , val_ply = val_tag , train_ply = train_tag , tn = 7 )\n",
    "save_to_pickle( sim_ply , 'candi/res_ver34_tag.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3></h3>\n",
    "<h3></h3>\n",
    "\n",
    "\n",
    "**genre matrix 만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노래와 장르 dictionary 화\n",
    "def song_to_gnr ( df ):\n",
    "    global song_meta\n",
    "    d_g = song_meta.song_gn_dtl_gnr_basket.values\n",
    "    g = song_meta.song_gn_gnr_basket.values\n",
    "\n",
    "    song_gnr = dict(zip(song_meta.id , [ d_g[i] + g[i]  for i in range(len(d_g))]) )\n",
    "    gnrs = [[ [g for s in sgs for g in song_gnr[s]] ] for sgs in df.songs.values]\n",
    "    return pd.DataFrame( gnrs , columns = ['gnr'] , index = df.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 장르와 노래 맵핑\n",
    "def get_genre_matrix( df ):\n",
    "    global genre\n",
    "    gnr_new = pd.DataFrame(list(zip(genre.index , range(len(genre.index)))) , columns = ['id' , 'label'])\n",
    "    gnrs = song_to_gnr( df )\n",
    "\n",
    "    res = get_cluster_matrix( gnrs.reset_index() , gnr_new , by = 'genre')\n",
    "    res.columns = ['id'] + list(genre.values)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 병합과정\n",
    "g = pd.concat( [train , val] )\n",
    "g = pd.concat( [g , test ] )\n",
    "\n",
    "\n",
    "total_gnr = get_genre_matrix(g)\n",
    "\n",
    "val_gnr = total_gnr.loc[ total_gnr.id.isin( list(val.id.values) ) ] # singer \n",
    "train_gnr = total_gnr.loc[ total_gnr.id.isin( list(train.id.values) ) ] # sin\n",
    "\n",
    "#최종!\n",
    "# train_gnr = total_gnr.loc[ total_gnr.id.isin( list(train.id.values) ) ] # sin\n",
    "# del train_gnr # 메모리 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ply = dist_get_sim_ply( 0 ,  3000 , val_ply = val_gnr , train_ply = train_gnr , tn = 7 )\n",
    "save_to_pickle( sim_ply , 'candi/res_ver34_gnr.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3></h3>\n",
    "<h3></h3>\n",
    "\n",
    "**album matrix 만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df = pd.read_pickle('data/transfromation/album_df.pickle')\n",
    "album_vec = pd.read_pickle('data/song_clustering/clust_album_emb100.pickle')\n",
    "\n",
    "\n",
    "total_album = get_cluster_matrix( album_df , album_vec , by = 'album')\n",
    "train_album = total_album.loc[ total_album.id.isin( train.id.values ) ]\n",
    "val_album = total_album.loc[ total_album.id.isin( val.id.values ) ]\n",
    "\n",
    "# 최종 \n",
    "# test_album = album_df.loc[ album_df.id.isin( train.id.values ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ply = dist_get_sim_ply( 0 , 3000 , val_ply = val_album , train_ply = train_album )\n",
    "save_to_pickle( sim_ply , 'candi/res_ver34_album.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3></h3>\n",
    "<h3></h3>\n",
    "\n",
    "### 임의 추가적 비교 기준\n",
    "\n",
    "1. song + word + singer + gnr + tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ply = pd.merge( train_song , train_word , on = 'id' , how = 'inner')\n",
    "train_ply = pd.merge( train_ply , train_singer , on = 'id' , how = 'inner')\n",
    "train_ply = pd.merge( train_ply , train_gnr , on = 'id' , how = 'inner')\n",
    "train_ply = pd.merge( train_ply , train_tag , on = 'id' , how = 'inner')\n",
    "train_ply = pd.merge( train_ply , train_album , on = 'id' , how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ply = pd.merge( val_song , val_word , on = 'id' , how = 'inner')\n",
    "val_ply = pd.merge( val_ply , val_singer , on = 'id' , how = 'inner')\n",
    "val_ply = pd.merge( val_ply , val_gnr , on = 'id' , how = 'inner')\n",
    "val_ply = pd.merge( val_ply , val_tag , on = 'id' , how = 'inner')\n",
    "val_ply = pd.merge( val_ply , val_album , on = 'id' , how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ply = dist_get_sim_ply( 0 ,  3000 , val_ply = val_ply , train_ply = train_ply , tn = 7)\n",
    "save_to_pickle( sim_ply , 'candi/res_ver34_ply.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 105.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 105.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 103.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 105.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 103.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 107.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 109.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015it [00:19, 105.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 만큼 했다\n"
     ]
    }
   ],
   "source": [
    "train_SAG = pd.merge( train_singer , train_album , on = 'id' , how = 'inner' )\n",
    "train_SAG = pd.merge( train_SAG , train_gnr , on = 'id' , how = 'inner' )\n",
    "\n",
    "val_SAG = pd.merge( val_singer , val_album , on = 'id' , how = 'inner' )\n",
    "val_SAG = pd.merge( val_SAG , val_gnr , on = 'id' , how = 'inner' )\n",
    "\n",
    "sim_ply = dist_get_sim_ply( 0 ,  3000 , val_ply = val_SAG , train_ply = train_SAG , tn = 7)\n",
    "save_to_pickle( sim_ply , 'candi/res_ver37_SAG.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:26, 111.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 108.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 108.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 110.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 107.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 108.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 110.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015it [00:17, 111.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 만큼 했다\n"
     ]
    }
   ],
   "source": [
    "train_SWG = pd.merge( train_song , train_word , on = 'id' , how = 'inner' )\n",
    "train_SWG = pd.merge( train_SWG , train_gnr , on = 'id' , how = 'inner' )\n",
    "\n",
    "val_SWG = pd.merge( val_song , val_word , on = 'id' , how = 'inner' )\n",
    "val_SWG = pd.merge( val_SWG , val_gnr , on = 'id' , how = 'inner' )\n",
    "\n",
    "sim_ply = dist_get_sim_ply( 0 ,  3000 , val_ply = val_SWG , train_ply = train_SWG , tn = 7)\n",
    "save_to_pickle( sim_ply , 'candi/res_ver37_SWG.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 110.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 110.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 107.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 105.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:26, 113.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:26, 112.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 110.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015it [00:19, 105.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 만큼 했다\n"
     ]
    }
   ],
   "source": [
    "train_SWAG = pd.merge( train_SWG , train_album , on = 'id' , how = 'inner')\n",
    "val_SWAG = pd.merge( val_SWG , val_album , on = 'id' , how = 'inner')\n",
    "\n",
    "sim_ply = dist_get_sim_ply( 0 ,  3000 , val_ply = val_SWAG , train_ply = train_SWAG , tn = 7)\n",
    "save_to_pickle( sim_ply , 'candi/res_ver39_SWAG.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:26, 113.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:26, 111.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:26, 112.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:26, 111.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 108.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:26, 112.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:26, 112.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015it [00:17, 112.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 만큼 했다\n"
     ]
    }
   ],
   "source": [
    "train_TSA = pd.merge( train_tag , train_singer , on = 'id' , how = 'inner' )\n",
    "train_TSA = pd.merge( train_TSA , train_album , on = 'id' , how = 'inner' )\n",
    "\n",
    "val_TSA = pd.merge( val_tag , val_singer , on = 'id' , how = 'inner' )\n",
    "val_TSA = pd.merge( val_TSA , val_album , on = 'id' , how = 'inner' )\n",
    "\n",
    "sim_ply = dist_get_sim_ply( 0 ,  3000 , val_ply = val_TSA , train_ply = train_TSA , tn = 7)\n",
    "save_to_pickle( sim_ply , 'candi/res_ver40_TSA.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 108.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 110.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 110.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 104.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 107.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 107.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:27, 108.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015it [00:18, 108.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 만큼 했다\n"
     ]
    }
   ],
   "source": [
    "train_WAT = pd.merge( train_tag , train_singer , on = 'id' , how = 'inner' )\n",
    "train_WAT = pd.merge( train_WAT , train_word , on = 'id' , how = 'inner' )\n",
    "\n",
    "val_WAT = pd.merge( val_tag , val_singer , on = 'id' , how = 'inner' )\n",
    "val_WAT = pd.merge( val_WAT , val_word , on = 'id' , how = 'inner' )\n",
    "\n",
    "sim_ply = dist_get_sim_ply( 0 ,  3000 , val_ply = val_WAT , train_ply = train_WAT , tn = 7)\n",
    "save_to_pickle( sim_ply , 'candi/res_ver40_WAT.pickle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 106.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:30, 99.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:29, 102.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 105.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 106.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 104.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:28, 107.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015it [00:18, 107.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 만큼 했다\n"
     ]
    }
   ],
   "source": [
    "train_SWAT = pd.merge( train_WAT , train_song , on = 'id' , how = 'inner' )\n",
    "val_SWAT = pd.merge( val_WAT , val_song , on = 'id' , how = 'inner' )\n",
    "\n",
    "sim_ply = dist_get_sim_ply( 0 ,  3000 , val_ply = val_SWAT , train_ply = train_SWAT , tn = 7)\n",
    "save_to_pickle( sim_ply , 'candi/res_ver40_SWAT.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
