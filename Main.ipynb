{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  함수 설명\n",
    "\n",
    "- `KakaoArena.Get_df` : 플레이리스트 id / 컬럼 전처리 후 데이터 프레임\n",
    "- `KakaoArena.Merged_table` : 클러스터링 전 , 테이블 합치기\n",
    "- `KakaoArena.Clustering_km` : word2vec 후 kmeans 로 클러스터링\n",
    "- `KakaoArena.Get_cluster_matrix` : 메트릭스를 만들기\n",
    "- `KakaoArena.Select_candidate` : 후보 플레이리스트 만들기\n",
    "- `KakaoArena.Get_candidate` : 추천할 노래와 태그를 가져오는 함수\n",
    "- `KakaoArena.Filter_date` : 날짜를 필터해주는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import KakaoArena.Filter_date as fdate\n",
    "import KakaoArena.Merged_table as mt # 후보군 테이블 merge \n",
    "import KakaoArena.Clustering_km as km\n",
    "import KakaoArena.Get_cluster_matrix as gcm\n",
    "import KakaoArena.Select_candidate as sc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "genre = pd.read_json('data/genre_gn_all.json', typ = 'series', encoding = \"utf-8\")\n",
    "meta = pd.read_json('data/song_meta.json', typ = 'frame', encoding=\"utf-8\")\n",
    "train = pd.read_json('data/train.json', typ = 'frame',encoding=\"utf-8\")\n",
    "val = pd.read_json('data/val.json', typ = 'frame',encoding=\"utf-8\")\n",
    "test = pd.read_json('data/test.json', typ = 'frame',encoding=\"utf-8\")\n",
    "\n",
    "complex_ = pd.concat([ train , val] , axis = 0 )\n",
    "complex_ = pd.concat([ complex_ , test ] , axis = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# meta data  date 필터링 해주기 \n",
    "filter_date = fdate.Filter_date( complex_ , meta )\n",
    "meta = filter_date.kill_zeros()\n",
    "\n",
    "# meta.to_pickle(\"data/done_filter_meta.pickle\")\n",
    "# meta = pd.read_pickle( 'data/done_filter_meta.pickle') # 한번했으면 이것만 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_title_merge\n"
     ]
    }
   ],
   "source": [
    "merged_t = mt.Merged_table( complex_, genre, meta )\n",
    "clu_km = km.Clustering( complex_, meta )\n",
    "\n",
    "tag_title_df = merged_t.tag_title( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 클러스터링 하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_title_merge\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-676515f1f51d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 태그와 타이틀 로 이루어진 데이터 프레임 만드는 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtag_title_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtag_title_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/tag_title_df.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\workdiratory\\melon\\code_melon\\KakaoArena\\Merged_table.py\u001b[0m in \u001b[0;36mtag_title\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# tar , title 함수 다 넣기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mget_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGet_data\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomplex_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenre\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mid_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_title_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mid_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomplex_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tags\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\workdiratory\\melon\\code_melon\\KakaoArena\\Get_df.py\u001b[0m in \u001b[0;36mget_title_df\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mstop_words_lst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mjuump\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjs_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0msent_lst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\workdiratory\\melon\\code_melon\\KakaoArena\\Get_df.py\u001b[0m in \u001b[0;36mjs_title\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# cmd를 Train시켜준다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mspm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentencePieceTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;31m# 생성된 model파일 불러오기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentencepiece\\__init__.py\u001b[0m in \u001b[0;36mTrain\u001b[1;34m(arg, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m       \u001b[1;34m\"\"\"Train Sentencepiece model. Accept both kwargs and legacy string arg.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_TrainFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "merged_t = mt.Merged_table( complex_, genre, meta )\n",
    "clu_km = km.Clustering( complex_, meta )\n",
    "\n",
    "# 태그와 타이틀 로 이루어진 데이터 프레임 만드는 함수\n",
    "tag_title_df = merged_t.tag_title()\n",
    "tag_title_df.to_pickle(\"data/tag_title_df.pickle\")\n",
    "\n",
    "\n",
    "# A. Song  : complex_\n",
    "song_clu = clu_km.clustering_song( clu = 200 )\n",
    "song_clu.to_pickle(\"data/clu_song_emb_200.pickle\")\n",
    "\n",
    "# B. word \n",
    "tag_gnr_title_clu = clu_km.clustering_( merged_t.tag_gnr_title() , clu = 100)\n",
    "tag_gnr_title_clu.to_pickle(\"data/clu_tag_gnr_title_emb_100.pickle\")\n",
    "\n",
    "# C. singer \n",
    "singer_clu = clu_km.clustering_( merged_t.title_singer() , clu = 100)\n",
    "singer_clu.to_pickle(\"data/clu_singer_emb_100.pickle\")\n",
    "\n",
    "\n",
    "# D. tag\n",
    "tag_clu = clu_km.clustering_tag( clu = 30  )\n",
    "tag_clu.to_pickle(\"data/clu_tag_emb_30.pickle\")\n",
    "\n",
    "\n",
    "# E. dtl_genre 원핫 인코딩을 했으므로 이 단계 생략\n",
    "\n",
    "\n",
    "# F . album  아래에서 df 랑 clustering이랑 코드가 합쳐져있음. \n",
    "emb_df , album_df  = clu_km.clustering_album( clu = 100 )\n",
    "emb_df.to_pickle('data/clu_album_emb100.pickle' )\n",
    "album_df.to_pickle('data/album_df_.pickle') # 데이타 프레임 윗단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matrix와 후보 플레이리스트 선별하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = gcm.Get_cluster_matrix( complex_, meta, genre )\n",
    "candidate = sc.Select_candidate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. song matrix & candidate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:13, 229.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:13, 226.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:13, 224.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:13, 227.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:13, 224.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:13, 226.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:13, 227.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000 만큼 했다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015it [00:08, 226.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 만큼 했다\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'candi/res_ver35_song.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6fd37f6a2914>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msim_ply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist_get_sim_ply\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[1;36m3000\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mval_ply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_song\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtrain_ply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_song\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mcandidate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_to_pickle\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msim_ply\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"candi/res_ver35_song.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\workdiratory\\melon\\code_melon\\KakaoArena\\Select_candidate.py\u001b[0m in \u001b[0;36msave_to_pickle\u001b[1;34m(self, ob, f_path)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m# 피클로 저장하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_to_pickle\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mob\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mf_path\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mf_path\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mob\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'candi/res_ver35_song.pickle'"
     ]
    }
   ],
   "source": [
    "song_vec = pd.read_pickle( 'data/clu_song_emb_200.pickle') # train + val + test 기반 임베딩\n",
    "song_vec = song_vec[['song_id' , 'label'] + list(song_vec.columns[3:12])]\n",
    "\n",
    "train_song = matrix.get_cluster_matrix( train , song_vec , by = \"song\")\n",
    "val_song = matrix.get_cluster_matrix( val , song_vec , by = \"song\")\n",
    "\n",
    "# # 최종!\n",
    "# # test_song = gcm.get_cluster_matrix( test , song_vec  , by = 'song' ) \n",
    "\n",
    "sim_ply = candidate.dist_get_sim_ply( 0 ,  3000 , val_ply = val_song , train_ply = train_song , tn = 30 )\n",
    "candidate.save_to_pickle( sim_ply , \"candi/res_ver35_song.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. word(gnr+tag+title) matrix & candidate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#임베딩된 개별 단어 df\n",
    "word_vec = pd.read_pickle( 'data/clu_tag_gnr_title_emb_100.pickle' )\n",
    "\n",
    "total_word = matrix.get_cluster_matrix( merged_t.tag_gnr_title() , word_vec , by = 'word')\n",
    "\n",
    "val_word = total_word.loc[ total_word.id.isin( list(val.id.values) ) ] # word ( val )\n",
    "train_word = total_word.loc[ total_word.id.isin( list(train.id.values ) ) ] # word ( \n",
    "\n",
    "# 최종\n",
    "#test_word = total_word.loc[ total_word.id.isin( list(test.id.values ) ) ]\n",
    "# del total_word #( 메모리 최적화를 위해 )\n",
    "\n",
    "sim_ply = candidate.dist_get_sim_ply( 0 ,  3000 , val_ply = val_word , train_ply = train_word , tn = 30 )\n",
    "candidate.save_to_pickle( sim_ply , 'candi/res_ver40_word.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. singer matrix & candidate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#임베딩된 개별 단어,가수\n",
    "singer_vec = pd.read_pickle( 'data/clu_singer_emb_100.pickle')\n",
    "\n",
    "total_singer = matrix.get_cluster_matrix( merged_t.title_singer() , singer_vec , by = 'word')\n",
    "val_singer = total_singer.loc[ total_singer.id.isin( list(val.id.values) ) ] # singer \n",
    "train_singer = total_singer.loc[ total_singer.id.isin( list(train.id.values) ) ] # sin\n",
    "\n",
    "#최종!\n",
    "# test_singer = total_singer.loc[ total_singer.id.isin( list(test.id.values) ) ] \n",
    "# del total_word # 메모리 최적화\n",
    "\n",
    "\n",
    "sim_ply = candidate.dist_get_sim_ply( 0 ,  3000 , val_ply = val_singer , train_ply = train_singer , tn = 30 )\n",
    "candidate.save_to_pickle( sim_ply , 'candi/res_ver40_singer.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. tag matrix & candidate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태그로만 이루어진 ply df\n",
    "tag_df = pd.read_pickle('tag_df.pickle') # clustring_tag 함수를 실행시키면 자동으로 저장됨\n",
    "\n",
    "# 임베딩된 태그들\n",
    "tag_vec = pd.read_pickle('data/clu_tag_emb_30.pickle') # train , val , test 모두 합쳐서 embeding\n",
    "\n",
    "total_tag = matrix.get_cluster_matrix( tag_df , tag_vec , by = 'tag')\n",
    "val_tag = total_tag.loc[ total_tag.id.isin( list(val.id.values) )]\n",
    "train_tag = total_tag.loc[ total_tag.id.isin( list(train.id.values) )]\n",
    "\n",
    "#최종\n",
    "# test_tag = total_tag.loc[ total_tag.id.isin( list(test.id.values) )]\n",
    "\n",
    "sim_ply = candidate.dist_get_sim_ply( 0 ,  3000 , val_ply = val_tag , train_ply = train_tag , tn = 30 )\n",
    "candidate.save_to_pickle( sim_ply , 'candi/res_ver40_tag.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E. genre matrix & candidate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_gnr = matrix.get_genre_matrix( complex_ ,meta, genre )\n",
    "val_gnr = total_gnr.loc[ total_gnr.id.isin( list(val.id.values) ) ] \n",
    "train_gnr = total_gnr.loc[ total_gnr.id.isin( list(train.id.values) ) ] \n",
    "\n",
    "#최종!\n",
    "# train_gnr = total_gnr.loc[ total_gnr.id.isin( list(train.id.values) ) ] # \n",
    "# del train_gnr # 메모리 최적화\n",
    "\n",
    "sim_ply = candidate.dist_get_sim_ply( 0 ,  3000 , val_ply = val_gnr , train_ply = train_gnr , tn = 30 )\n",
    "candidate.save_to_pickle( sim_ply , 'candi/res_ver40_gnr.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F. album matrix & candidate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df = pd.read_pickle('data/album_df.pickle')\n",
    "album_vec = pd.read_pickle('data/clu_album_emb100.pickle')\n",
    "\n",
    "\n",
    "total_album = matrix.get_cluster_matrix( album_df , album_vec , by = 'album')\n",
    "train_album = total_album.loc[ total_album.id.isin( train.id.values ) ]\n",
    "val_album = total_album.loc[ total_album.id.isin( val.id.values ) ]\n",
    "\n",
    "# 최종 \n",
    "# test_album = album_df.loc[ album_df.id.isin( train.id.values ) ]\n",
    "\n",
    "sim_ply = candidate.dist_get_sim_ply( 0 , 3000 , val_ply = val_album , train_ply = train_album ,tn = 30 )\n",
    "candidate.save_to_pickle( sim_ply , 'candi/res_ver40_album.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**G. Ply matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ply matrix \n",
    "\n",
    "train_ply = pd.merge( train_song , train_word , on = 'id' , how = 'inner')\n",
    "train_ply = pd.merge( train_ply , train_singer , on = 'id' , how = 'inner')\n",
    "train_ply = pd.merge( train_ply , train_gnr , on = 'id' , how = 'inner')\n",
    "train_ply = pd.merge( train_ply , train_tag , on = 'id' , how = 'inner')\n",
    "train_ply = pd.merge( train_ply , train_album , on = 'id' , how = 'inner')\n",
    "\n",
    "val_ply = pd.merge( val_song , val_word , on = 'id' , how = 'inner')\n",
    "val_ply = pd.merge( val_ply , val_singer , on = 'id' , how = 'inner')\n",
    "val_ply = pd.merge( val_ply , val_gnr , on = 'id' , how = 'inner')\n",
    "val_ply = pd.merge( val_ply , val_tag , on = 'id' , how = 'inner')\n",
    "val_ply = pd.merge( val_ply , val_album , on = 'id' , how = 'inner')\n",
    "\n",
    "sim_ply = candidate.dist_get_sim_ply( 0 ,  3000 , val_ply = val_ply , train_ply = train_ply , tn = 30 )\n",
    "candidate.save_to_pickle( sim_ply , 'candi/res_ver40_ply.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 후보 플레이리스트에서 후보 노래와 태그를 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import KakaoArena.Get_candidate as gc\n",
    "get_songtag = gc.Get_candidate(train, tag_title_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 저장한 pickle 을 불러오기\n",
    "\n",
    "s_sim = candidate.load_pickle( 'candi/res_ver35_song.pickle' )\n",
    "w_sim = candidate.load_pickle( 'candi/res_ver35_word.pickle' )\n",
    "sin_sim = candidate.load_pickle( 'candi/res_ver35_singer.pickle' )\n",
    "t_sim = candidate.load_pickle( 'candi/res_ver35_tag.pickle' )\n",
    "g_sim = candidate.load_pickle( 'candi/res_ver35_gnr.pickle' )\n",
    "\n",
    "ply_sim = candidate.load_pickle( 'candi/res_ver35_ply.pickle' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**후보곡과 태그를 뽑은 후, 날짜를 필터링해 줌**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_can = get_songtag.get_candidate( s_sim , val, by =\"set\" )\n",
    "s_can = fdate.filter_date( s_can )\n",
    "\n",
    "w_can = get_songtag.get_candidate( w_sim , val, by =\"set\" )\n",
    "w_can = fdate.filter_date( w_can  )\n",
    "\n",
    "sin_can = get_songtag.get_candidate( sin_sim , val, by =\"set\" )\n",
    "sin_can = fdate.filter_date( sin_can)\n",
    "\n",
    "t_can = get_songtag.get_candidate( t_sim , val, by =\"set\")\n",
    "t_can = fdate.filter_date( t_can )\n",
    "\n",
    "g_can = get_songtag.get_candidate( g_sim , val, by =\"set\")\n",
    "g_can = fdate.filter_date( g_can )\n",
    "\n",
    "ply_can = get_songtag.get_candidate( ply_sim , val, by =\"set\" )\n",
    "ply_can = fdate.filter_date( ply_can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
